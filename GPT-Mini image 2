import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from PIL import Image
import numpy as np

# ==== Dataset com Hugging Face (COCO exemplo) ====
class GPTMiniImageDataset(Dataset):
    def __init__(self, split="train", resolution=11520):
        self.dataset = load_dataset("coco", split=split)
        self.resolution = resolution

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        sample = self.dataset[idx]
        # Imagem
        img = sample["image"].convert("RGB")
        img = img.resize((self.resolution, self.resolution))
        img_tensor = torch.tensor(np.array(img), dtype=torch.float32).permute(2,0,1)/255.0
        # Tokens dummy (pode ser substituído por tokenizer real)
        text_tokens = torch.randint(0, 8192, (256,))
        return text_tokens, img_tensor

# ==== VAE para compactação de imagens ====
class GPTMiniImageVAE(nn.Module):
    def __init__(self, image_size=11520, dim=1024):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, dim, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim, dim*2, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*2, dim*4, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*4, dim*8, 4, 2, 1), nn.ReLU()
        )
        self.to_latent = nn.Conv2d(dim*8, dim, 1)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(dim, dim*8, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*8, dim*4, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*4, dim*2, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*2, dim, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim, 3, 4, 2, 1)
        )

    def encode(self, x):
        return self.to_latent(self.encoder(x))

    def decode(self, z):
        return self.decoder(z)

# ==== Transformer avançado ====
class AdvancedTransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_hidden):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_hidden),
            nn.GELU(),
            nn.Linear(ff_hidden, embed_dim)
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_out,_ = self.attn(x,x,x)
        x = self.norm1(x + attn_out)
        x = self.norm2(x + self.ff(x))
        return x

# ==== GPT-Mini Image 2 ====
class GPTMiniImage2(nn.Module):
    def __init__(self, vae, num_tokens=8192, embed_dim=1024, depth=24, num_heads=16):
        super().__init__()
        self.vae = vae
        self.num_tokens = num_tokens
        self.embed_dim = embed_dim
        self.token_embedding = nn.Embedding(num_tokens, embed_dim)
        self.pos_embedding = nn.Parameter(torch.randn(1, 256, embed_dim))
        self.transformer_blocks = nn.ModuleList(
            [AdvancedTransformerBlock(embed_dim, num_heads, embed_dim*4) for _ in range(depth)]
        )
        self.to_latent = nn.Linear(embed_dim, embed_dim)
        self.to_image = nn.Linear(embed_dim, 3*vae.encoder[0].in_channels*vae.encoder[0].kernel_size[0]**2)

    def forward(self, text_tokens, image=None):
        text_emb = self.token_embedding(text_tokens)
        x = text_emb + self.pos_embedding[:, :text_tokens.size(0)]
        for block in self.transformer_blocks:
            x = block(x)
        latent = self.to_latent(x.mean(dim=0, keepdim=True))
        image_pred = self.vae.decode(latent.unsqueeze(-1).unsqueeze(-1))
        return image_pred

# ==== Treinamento simplificado ====
def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    vae = GPTMiniImageVAE().to(device)
    model = GPTMiniImage2(vae).to(device)

    dataset = GPTMiniImageDataset(split="train")
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()

    for epoch in range(5):
        for text, image in dataloader:
            text, image = text.to(device), image.to(device)
            optimizer.zero_grad()
            output = model(text, image)
            loss = criterion(output, image)
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

    print("✅ Treinamento concluído!")

if __name__ == "__main__":
    train()
